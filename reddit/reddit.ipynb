{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qond2Rae1NI"
      },
      "outputs": [],
      "source": [
        "pip install openai tenacity seaborn praw --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas re typing"
      ],
      "metadata": {
        "id": "YapfDbrKfjlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import config\n",
        "import praw\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Generator, Optional\n",
        "import re\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "\n",
        "openai.api_key = config.OPENAI_API_KEY\n",
        "\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id=config.REDDIT_CLIENT_ID,\n",
        "    client_secret=config.REDDIT_CLIENT_SECRET,\n",
        "    user_agent=f\"script:test:0.0.1 (by u/Charming_Sale2064)\",\n",
        ")\n",
        "# print(reddit.read_only)\n",
        "\n",
        "# for submission in reddit.subreddit(\"test\").hot(limit=10):\n",
        "#     print(submission.title)\n",
        "\n",
        "DF_COLUMNS = [\"subreddit\", \"submission_id\", \"score\", \"comment_body\"]\n",
        "filename, subreddits = (\n",
        "    \"epl_top_8.csv\",\n",
        "    [\n",
        "        \"reddevils\",\n",
        "        \"LiverpoolFC\",\n",
        "        \"chelseafc\",\n",
        "        \"Gunners\",\n",
        "        \"coys\",\n",
        "        \"MCFC\",\n",
        "        \"Everton\",\n",
        "        \"NUFC\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Utility functions for fetching comments from submissions\n",
        "def comment_generator(submission) -> Generator:\n",
        "    # Do not bother expanding MoreComments (follow-links)\n",
        "    for comment in submission.comments.list():\n",
        "        if (\n",
        "            hasattr(comment, \"body\")\n",
        "            and comment.body != \"[deleted]\"\n",
        "            and comment.body != \"[removed]\"\n",
        "        ):\n",
        "            yield (comment)\n",
        "\n",
        "def collect_comments(\n",
        "    filename: str,\n",
        "    target_comments_per_subreddit: int,\n",
        "    max_comments_per_submission: int,\n",
        "    max_comment_length: int,\n",
        "    reddit: praw.Reddit,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Collect comments from the top submissions in each subreddit.\n",
        "\n",
        "    Cache results at cache_filename.\n",
        "\n",
        "    Return a dataframe with columns: subreddit, submission_id, score, comment_body\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filename, index_col=\"id\")\n",
        "        assert df.columns.tolist() == DF_COLUMNS\n",
        "    except FileNotFoundError:\n",
        "        df = pd.DataFrame(columns=DF_COLUMNS)\n",
        "\n",
        "    # dict like {comment_id -> {column -> value}}\n",
        "    records = df.to_dict(orient=\"index\")\n",
        "\n",
        "    for subreddit_index, subreddit_name in enumerate(subreddits):\n",
        "        print(f\"Processing Subreddit: {subreddit_name}\")\n",
        "\n",
        "        processed_comments_for_subreddit = len(df[df[\"subreddit\"] == subreddit_name])\n",
        "\n",
        "        if processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
        "            print(\n",
        "                f\"Enough comments fetched for {subreddit_name}, continuing to next subreddit.\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        # `top`` is a generator, grab submissions until we break (within this loop).\n",
        "        for submission in reddit.subreddit(subreddit_name).top(time_filter=\"month\"):\n",
        "            if processed_comments_for_subreddit >= target_comments_per_subreddit:\n",
        "                break\n",
        "\n",
        "            # The number of comments that we already have for this subreddit\n",
        "            processed_comments_for_submission = len(\n",
        "                df[df[\"submission_id\"] == submission.id]\n",
        "            )\n",
        "\n",
        "            for comment in comment_generator(submission):\n",
        "                if (\n",
        "                    processed_comments_for_submission >= max_comments_per_submission\n",
        "                    or processed_comments_for_subreddit >= target_comments_per_subreddit\n",
        "                ):\n",
        "                    break\n",
        "\n",
        "                if comment.id in records:\n",
        "                    print(\n",
        "                        f\"Skipping comment {subreddit_name}-{submission.id}-{comment.id} because we already have it\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                body = comment.body[:max_comment_length].strip()\n",
        "                records[comment.id] = {\n",
        "                    \"subreddit\": subreddit_name,\n",
        "                    \"submission_id\": submission.id,\n",
        "                    \"comment_body\": body,\n",
        "                }\n",
        "\n",
        "                processed_comments_for_subreddit += 1\n",
        "                processed_comments_for_submission += 1\n",
        "\n",
        "            # Once per post write to disk.\n",
        "            print(f\"CSV rewritten with {len(records)} rows.\\n\")\n",
        "            df = pd.DataFrame.from_dict(records, orient=\"index\", columns=DF_COLUMNS)\n",
        "            df.to_csv(filename, index_label=\"id\")\n",
        "\n",
        "    print(\"Completed.\")\n",
        "    return df\n",
        "\n",
        "MAX_ATTEMPTS = 3\n",
        "\n",
        "\n",
        "def generate_prompt_messages(s: str) -> List[Dict]:\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"\n",
        "The following is a comment from a user on Reddit. Score it from -1 to 1, where -1 is the most negative and 1 is the most positive:\n",
        "\n",
        "The traffic is quite annoying.\n",
        "\"\"\".strip(),\n",
        "        },\n",
        "        {\"role\": \"assistant\", \"content\": \"-0.75\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"\n",
        "The following is a comment from a user on Reddit. Score it from -1 to 1, where -1 is the most negative and 1 is the most positive:\n",
        "\n",
        "The library is downtown.\n",
        "\"\"\".strip(),\n",
        "        },\n",
        "        {\"role\": \"assistant\", \"content\": \"0.0\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\"\"\n",
        "The following is a comment from a user on Reddit. Score it from -1 to 1, where -1 is the most negative and 1 is the most positive:\n",
        "\n",
        "Even though it's humid, I really love the summertime. Everything is so green and the sun is out all the time.\n",
        "\"\"\".strip(),\n",
        "        },\n",
        "        {\"role\": \"assistant\", \"content\": \"0.8\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"\n",
        "The following is a comment from a user on Reddit. Score it from -1 to 1, where -1 is the most negative and 1 is the most positive:\n",
        "\n",
        "{s}\n",
        "\"\"\".strip(),\n",
        "        },\n",
        "    ]\n",
        "\n",
        "class UnscorableCommentError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    wait_random_exponential,\n",
        "    retry_if_exception_type,\n",
        "    stop_after_attempt,\n",
        ")\n",
        "\n",
        "\n",
        "@retry(\n",
        "    wait=wait_random_exponential(multiplier=1, max=30),\n",
        "    stop=stop_after_attempt(3),\n",
        "    retry=retry_if_exception_type(UnscorableCommentError)\n",
        "    | retry_if_exception_type(openai.APIConnectionError)\n",
        "    | retry_if_exception_type(openai.APIError)\n",
        "    | retry_if_exception_type(openai.RateLimitError),\n",
        "    reraise=True,  # Reraise the last exception\n",
        ")\n",
        "def score_sentiment(s: str, model: str) -> float:\n",
        "    messages = generate_prompt_messages(s)\n",
        "    response = openai.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "    )\n",
        "    score_response = response.choices[0].message.content.strip()\n",
        "    # This will raise an Attribute Error if the regular expression doesn't match\n",
        "    try:\n",
        "        return float(re.search(r\"([-+]?\\d*\\.?\\d+)\", score_response).group(1))\n",
        "    except AttributeError:\n",
        "        raise UnscorableCommentError(f\"Could not score comment: {s}\")\n",
        "\n",
        "\n",
        "def score_sentiments(filename: str, model: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Score sentiments contained in comments in filename.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(filename, index_col=\"id\")\n",
        "    assert df.columns.tolist() == DF_COLUMNS\n",
        "\n",
        "    records = df.to_dict(orient=\"index\")\n",
        "\n",
        "    for index, item in enumerate(records.items()):\n",
        "        comment_id, comment = item\n",
        "\n",
        "        if not pd.isna(comment[\"score\"]):\n",
        "            print(f\"{comment_id} was already scored. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        body = comment[\"comment_body\"]\n",
        "        try:\n",
        "            score = score_sentiment(body, model=model)\n",
        "        except UnscorableCommentError:\n",
        "            # The score_sentiment method will retry 3 times before letting this error pass through.\n",
        "            # If it does, we will consider this comment un-processable and skip it.\n",
        "            # For other errors, such as APIConnectionError, we will fail completely and let the user know.\n",
        "            continue\n",
        "        print(\n",
        "            f\"\"\"\n",
        "            {comment_id} - ({index + 1} of {len(records)} Comments)\n",
        "            Body: {body[:80]}\n",
        "            Score: {score}\"\"\".strip()\n",
        "        )\n",
        "\n",
        "        records[comment_id][\"score\"] = score\n",
        "        df = pd.DataFrame.from_dict(records, orient=\"index\", columns=DF_COLUMNS)\n",
        "        df.to_csv(filename, index_label=\"id\")\n",
        "\n",
        "    print(\"Scoring completed.\")\n",
        "    return df\n",
        "\n",
        "# Define our plotting function\n",
        "\n",
        "\n",
        "# https://seaborn.pydata.org/examples/kde_ridgeplot.html\n",
        "def get_avg_score_by_subreddit(dataframe):\n",
        "    \"\"\"\n",
        "    Given a pandas DataFrame with columns \"subreddit\" and \"score\", returns a new DataFrame\n",
        "    with the average score and standard deviation for each subreddit.\n",
        "    \"\"\"\n",
        "    # Group by subreddit and calculate the mean and standard deviation for each group\n",
        "    subreddit_stats = dataframe.groupby(\"subreddit\")[\"score\"].agg([\"mean\", \"std\"])\n",
        "\n",
        "    # Rename columns to indicate that they represent the mean and standard deviation\n",
        "    subreddit_stats.columns = [\"mean_score\", \"standard_deviation\"]\n",
        "\n",
        "    subreddit_stats = subreddit_stats.sort_values(\"mean_score\", ascending=True)\n",
        "\n",
        "    # Return the new DataFrame\n",
        "    return subreddit_stats\n",
        "\n",
        "\n",
        "def plot_sentiments(df):\n",
        "    sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
        "\n",
        "    # Create the data\n",
        "    df_scores = df[[\"score\", \"subreddit\"]]\n",
        "\n",
        "    # Initialize the FacetGrid object\n",
        "    pal = sns.cubehelix_palette(10, rot=-0.25, light=0.7)\n",
        "    g = sns.FacetGrid(\n",
        "        df_scores,\n",
        "        row=\"subreddit\",\n",
        "        row_order=get_avg_score_by_subreddit(df_scores).index.to_list(),\n",
        "        hue=\"subreddit\",\n",
        "        aspect=15,\n",
        "        height=0.5,\n",
        "        palette=pal,\n",
        "    )\n",
        "\n",
        "    # Draw the densities in a few steps\n",
        "    g.map(\n",
        "        sns.kdeplot,\n",
        "        \"score\",\n",
        "        bw_adjust=0.5,\n",
        "        clip_on=False,\n",
        "        fill=True,\n",
        "        alpha=1,\n",
        "        linewidth=1.5,\n",
        "    )\n",
        "    g.map(sns.kdeplot, \"score\", clip_on=False, color=\"w\", lw=2, bw_adjust=0.5)\n",
        "\n",
        "    # passing color=None to refline() uses the hue mapping\n",
        "    g.refline(y=0, linewidth=2, linestyle=\"-\", color=None, clip_on=False)\n",
        "\n",
        "    # Define and use a simple function to label the plot in axes coordinates\n",
        "    def label(x, color, label):\n",
        "        ax = plt.gca()\n",
        "        ax.text(\n",
        "            0,\n",
        "            0.2,\n",
        "            label,\n",
        "            fontweight=\"bold\",\n",
        "            color=color,\n",
        "            ha=\"left\",\n",
        "            va=\"center\",\n",
        "            transform=ax.transAxes,\n",
        "        )\n",
        "\n",
        "    g.map(label, \"score\")\n",
        "\n",
        "    # Set the subplots to overlap\n",
        "    g.figure.subplots_adjust(hspace=-0.25)\n",
        "\n",
        "    # Remove axes details that don't play well with overlap\n",
        "    g.set_titles(\"\")\n",
        "    g.set(yticks=[], ylabel=\"\")\n",
        "    g.despine(bottom=True, left=True)\n",
        "\n",
        "    # display(g.fig)\n",
        "\n",
        "NUM_SUBREDDITS = len(subreddits)\n",
        "TARGET_COMMENTS_PER_SUBREDDIT = 50\n",
        "MAX_COMMENTS_PER_SUBMISSION = 10\n",
        "MAX_COMMENT_LENGTH = 2000\n",
        "\n",
        "collect_comments(\n",
        "    filename=filename,\n",
        "    target_comments_per_subreddit=TARGET_COMMENTS_PER_SUBREDDIT,\n",
        "    max_comments_per_submission=MAX_COMMENTS_PER_SUBMISSION,\n",
        "    max_comment_length=MAX_COMMENT_LENGTH,\n",
        "    reddit=reddit,\n",
        ")\n",
        "\n",
        "df = score_sentiments(filename=filename, model=model)\n",
        "\n",
        "plot_sentiments(df)\n"
      ],
      "metadata": {
        "id": "IMQvG7rbfOQk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}